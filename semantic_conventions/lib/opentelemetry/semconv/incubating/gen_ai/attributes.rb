# frozen_string_literal: true

# Copyright The OpenTelemetry Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0
#
# This file was autogenerated. Do not edit it by hand.

module OpenTelemetry
  module SemConv
  module Incubating
    module GEN_AI
      # @!group Attribute Names
    
      # The full response received from the LLM.
      # 
      # It's RECOMMENDED to format completions as JSON string matching [OpenAI messages format](https://platform.openai.com/docs/guides/text-generation)
      # 
      # @note Stability Level: experimental
      #
      # @example Sample Values
      #   [{'role': 'assistant', 'content': 'The capital of France is Paris.'}]
      #
      GEN_AI_COMPLETION = 'gen_ai.completion'
  
      # The full prompt sent to an LLM.
      # 
      # It's RECOMMENDED to format prompts as JSON string matching [OpenAI messages format](https://platform.openai.com/docs/guides/text-generation)
      # 
      # @note Stability Level: experimental
      #
      # @example Sample Values
      #   [{'role': 'user', 'content': 'What is the capital of France?'}]
      #
      GEN_AI_PROMPT = 'gen_ai.prompt'
  
      # The maximum number of tokens the LLM generates for a request.
      # 
      # @note Stability Level: experimental
      #
      # @example Sample Values
      #   100
      #
      GEN_AI_REQUEST_MAX_TOKENS = 'gen_ai.request.max_tokens'
  
      # The name of the LLM a request is being made to.
      # 
      # @note Stability Level: experimental
      #
      # @example Sample Values
      #   gpt-4
      #
      GEN_AI_REQUEST_MODEL = 'gen_ai.request.model'
  
      # The temperature setting for the LLM request.
      # 
      # @note Stability Level: experimental
      #
      # @example Sample Values
      #   0.0
      #
      GEN_AI_REQUEST_TEMPERATURE = 'gen_ai.request.temperature'
  
      # The top_p sampling setting for the LLM request.
      # 
      # @note Stability Level: experimental
      #
      # @example Sample Values
      #   1.0
      #
      GEN_AI_REQUEST_TOP_P = 'gen_ai.request.top_p'
  
      # Array of reasons the model stopped generating tokens, corresponding to each generation received.
      # 
      # @note Stability Level: experimental
      #
      # @example Sample Values
      #   stop
      #
      GEN_AI_RESPONSE_FINISH_REASONS = 'gen_ai.response.finish_reasons'
  
      # The unique identifier for the completion.
      # 
      # @note Stability Level: experimental
      #
      # @example Sample Values
      #   chatcmpl-123
      #
      GEN_AI_RESPONSE_ID = 'gen_ai.response.id'
  
      # The name of the LLM a response was generated from.
      # 
      # @note Stability Level: experimental
      #
      # @example Sample Values
      #   gpt-4-0613
      #
      GEN_AI_RESPONSE_MODEL = 'gen_ai.response.model'
  
      # The Generative AI product as identified by the client instrumentation.
      # 
      # The actual GenAI product may differ from the one identified by the client. For example, when using OpenAI client libraries to communicate with Mistral, the `gen_ai.system` is set to `openai` based on the instrumentation's best knowledge.
      # 
      # @note Stability Level: experimental
      #
      # @example Sample Values
      #   openai
      #
      GEN_AI_SYSTEM = 'gen_ai.system'
  
      # The number of tokens used in the LLM response (completion).
      # 
      # @note Stability Level: experimental
      #
      # @example Sample Values
      #   180
      #
      GEN_AI_USAGE_COMPLETION_TOKENS = 'gen_ai.usage.completion_tokens'
  
      # The number of tokens used in the LLM prompt.
      # 
      # @note Stability Level: experimental
      #
      # @example Sample Values
      #   100
      #
      GEN_AI_USAGE_PROMPT_TOKENS = 'gen_ai.usage.prompt_tokens'
  
      # @!endgroup
    end
  end
  end
end